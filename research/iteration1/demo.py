#!/usr/bin/env python3
"""
Documentation Quality Automation - Complete Demo
This script demonstrates the full capabilities of the system.
"""

import os
import sys
import time
from pathlib import Path


def print_header(text):
    """Print formatted header."""
    print("\n" + "=" * 80)
    print(f"  {text}")
    print("=" * 80 + "\n")


def print_step(step, description):
    """Print step information."""
    print(f"\nüìç Step {step}: {description}")
    print("-" * 80)


def run_command(cmd, description=""):
    """Run command and show output."""
    if description:
        print(f"\n‚ñ∂Ô∏è  {description}")
    print(f"   Command: {cmd}\n")
    
    # In real usage, uncomment this:
    # os.system(cmd)
    
    # For demo, just show what would run
    print(f"   [Would execute: {cmd}]\n")
    time.sleep(0.5)


def main():
    """Run complete demonstration."""
    
    print_header("Documentation Quality Automation System - Complete Demo")
    
    print("""
This demo walks through the complete workflow:
1. Initial setup and configuration
2. Running comprehensive scans
3. Analyzing results and metrics
4. Applying auto-fixes
5. Monitoring and alerting
6. Generating reports and dashboards
7. CI/CD integration

Let's get started!
""")
    
    input("Press Enter to begin...")
    
    # Step 1: Setup
    print_step(1, "Initial Setup")
    print("""
First, let's verify the installation and configuration:
- Check Python dependencies
- Verify configuration file
- Initialize metrics database
""")
    
    run_command(
        "pip install -r requirements.txt",
        "Installing dependencies"
    )
    
    run_command(
        "python -c 'import yaml; print(yaml.safe_load(open(\"quality_config.yaml\")))'",
        "Validating configuration"
    )
    
    print("\n‚úÖ Setup complete!")
    input("\nPress Enter to continue...")
    
    # Step 2: Initial Scan
    print_step(2, "Running Comprehensive Documentation Scan")
    print("""
Now we'll scan the entire documentation repository for quality issues.
The system will check for:
- Terminology inconsistencies (deprecated terms)
- Missing frontmatter in MDX files
- Duplicate content paths
- Broken links
- Code examples without error handling
- Poor navigation structure
- And 15+ other issue types
""")
    
    run_command(
        "python doc_quality_automation.py scan --config quality_config.yaml",
        "Scanning documentation repository"
    )
    
    print("""
üìä Scan Results (Example):
   
   ‚úÖ Scan complete. Found 47 issues.
   
   Summary:
     CRITICAL: 2
     HIGH: 8
     MEDIUM: 23
     LOW: 14
   
   Top Issues:
   - 2 critical: Missing frontmatter in MDX files
   - 5 high: "Claude Code SDK" should be "Claude Agent SDK"
   - 3 high: Broken internal links
   - 8 medium: Code examples missing error handling
   - 15 low: Optimization opportunities
""")
    
    input("\nPress Enter to continue...")
    
    # Step 3: View Detailed Report
    print_step(3, "Generating Detailed Quality Report")
    
    run_command(
        "python doc_quality_automation.py report --output quality_report.md",
        "Generating comprehensive report"
    )
    
    print("""
üìÑ Report Generated: quality_report.md

The report includes:
‚úÖ Executive summary with key metrics
‚úÖ Trends over time (30-day comparison)
‚úÖ Issues organized by category and severity
‚úÖ Specific file locations and line numbers
‚úÖ Suggested fixes for each issue
‚úÖ Recommended action plan

Sample from report:
---
## Issues by Category

### Information Architecture (5)

**CRITICAL:** Duplicate content path. Canonical: /en/docs/agents-and-tools/agent-skills/
- File: `/en/api/skills/index.md`
- Fix: Convert to navigation pointer to canonical location

**HIGH:** Navigation depth violation (4 levels deep)
- File: `/en/docs/claude-code/third-party/bedrock/config.md`
- Fix: Promote to 3-level hierarchy
---
""")
    
    input("\nPress Enter to continue...")
    
    # Step 4: Collect Metrics
    print_step(4, "Collecting Quality Metrics and KPIs")
    
    run_command(
        "python doc_quality_automation.py metrics",
        "Collecting current metrics"
    )
    
    print("""
üìä Current Metrics:

Primary KPIs:
  critical_issues: 2
  high_issues: 8
  medium_issues: 23
  low_issues: 14
  total_issues: 47
  
  documentation_debt: 2 (target: <2) ‚úÖ
  auto_fixable_rate: 0.42 (42% of issues can be auto-fixed)
  
Secondary Metrics:
  total_files: 156
  issues_per_file: 0.30
  consistency_issues: 12
  completeness_issues: 15
  user_experience_issues: 8
  
Trends (30 days):
  total_issues: ‚Üì -23% (improving) ‚úÖ
  critical_issues: ‚Üì -67% (improving) ‚úÖ
  content_consistency_score: ‚Üë +12% (improving) ‚úÖ
""")
    
    input("\nPress Enter to continue...")
    
    # Step 5: Auto-Fix Issues
    print_step(5, "Applying Automated Fixes")
    print("""
The system can automatically fix many common issues:
- Replace deprecated terminology
- Add missing frontmatter
- Fix simple formatting issues
- Standardize case

Let's run auto-fix in dry-run mode first to see what would change:
""")
    
    run_command(
        "python doc_quality_automation.py fix --dry-run",
        "Dry-run: Preview changes without applying"
    )
    
    print("""
üîç Auto-fix Preview:

Would fix 20 issues across 12 files:

Terminology fixes (15 issues):
  ‚úì docs/api/overview.md: "Claude Code SDK" ‚Üí "Claude Agent SDK"
  ‚úì docs/api/quickstart.md: "Claude Code SDK" ‚Üí "Claude Agent SDK"
  ‚úì docs/sdk/typescript.md: "ClaudeCodeOptions" ‚Üí "ClaudeAgentOptions"
  ... (12 more files)

Frontmatter fixes (5 issues):
  ‚úì docs/guides/advanced.mdx: Add missing frontmatter block
  ‚úì docs/reference/errors.mdx: Add missing 'description' field
  ... (3 more files)

No files would be modified in dry-run mode.
""")
    
    print("\nLooks good! Now let's apply the fixes:")
    
    run_command(
        "python doc_quality_automation.py fix",
        "Applying automated fixes"
    )
    
    print("""
‚úÖ Auto-fix Complete!

Fixed 20 issues across 12 files:
  - Replaced deprecated terminology in 9 files
  - Added missing frontmatter to 3 files
  - Fixed formatting in 2 files

All changes backed up to: .doc_quality_backups/
All actions logged to: doc_quality.log

Remaining issues: 27 (require manual review)
""")
    
    input("\nPress Enter to continue...")
    
    # Step 6: Monitoring and Alerting
    print_step(6, "Setting Up Continuous Monitoring")
    print("""
The monitoring system provides:
- Continuous quality tracking
- Automated alerting when issues arise
- Trend analysis
- Interactive dashboards
""")
    
    run_command(
        "python doc_quality_monitoring.py",
        "Running monitoring cycle"
    )
    
    print("""
üìä Monitoring Cycle Complete:

Metrics collected: ‚úÖ
Trends analyzed: ‚úÖ
Alert rules evaluated: ‚úÖ
Dashboard generated: ‚úÖ

Alerts Triggered: 0

Quality Gates Status:
  ‚úÖ Critical Issues: 0 / 0 (PASS)
  ‚úÖ Documentation Debt: 2 / 2 (PASS)
  ‚úÖ Consistency Score: 0.89 / 0.95 (Warning - improving)

Dashboard available at: dashboard.html

Monitoring configured to run daily at 2 AM.
""")
    
    input("\nPress Enter to continue...")
    
    # Step 7: Dashboard
    print_step(7, "Interactive Dashboard")
    print("""
The HTML dashboard provides real-time visualization:

Key Features:
- üìä Live metrics display
- üìà 30-day trend charts
- üéØ Quality gate status
- üö® Open issues list
- üîÑ Auto-refresh every 5 minutes

Dashboard includes:
  
  [Executive Summary]
  Total Issues: 27
  Critical: 0 | High: 5 | Medium: 15 | Low: 7
  Auto-fixable: 18%
  
  [Trends]
  Total Issues: ‚Üì -42% (past 30 days)
  Critical Issues: ‚Üì -100% (past 30 days)
  
  [Quality Gates]
  ‚úÖ No Critical Issues
  ‚úÖ Documentation Debt < 2
  ‚ö†Ô∏è  Consistency Score (89% / 95%)
  
  [Top Issues]
  1. HIGH: Missing error handling in API examples (5 files)
  2. MEDIUM: Poor link text usage (8 instances)
  3. MEDIUM: Underutilized Mintlify components (6 opportunities)
  ...

Open dashboard.html in your browser to explore interactively!
""")
    
    input("\nPress Enter to continue...")
    
    # Step 8: CI/CD Integration
    print_step(8, "CI/CD Integration")
    print("""
The system integrates seamlessly with your development workflow:

GitHub Actions Integration:
- Automatic scans on every PR
- Quality gate enforcement (fails on critical issues)
- PR comments with results
- Auto-fix with automated PR creation
- Dashboard deployment to GitHub Pages

GitLab CI Integration:
- Pipeline quality checks
- Artifact reporting
- MR comments
- Scheduled scans

Local Git Hooks:
- Pre-commit quality validation
- Prevent commits with critical issues
""")
    
    print("\nTo enable GitHub Actions:")
    run_command(
        "cp .github-workflows-doc-quality.yml .github/workflows/doc-quality.yml",
        "Install GitHub Actions workflow"
    )
    
    print("""
‚úÖ CI/CD Integration Ready!

What happens on next PR:
1. Documentation changes detected
2. Quality scan runs automatically
3. Results posted as PR comment
4. Quality gates evaluated
5. PR blocked if critical issues found
6. Auto-fix can create follow-up PR

Example PR Comment:
---
## üìä Documentation Quality Report

**Scan Results:** 3 issues found

| Severity | Count |
|----------|-------|
| Critical | 0 ‚úÖ  |
| High     | 1     |
| Medium   | 2     |

**Quality Gates:** ‚úÖ All Passed

### Issues Found:
1. HIGH: Missing error handling in messages-example.py (line 23)
2. MEDIUM: Link text "here" is not descriptive (quickstart.md:45)

**Auto-fix available:** Run workflow to create PR with fixes
---
""")
    
    input("\nPress Enter to continue...")
    
    # Step 9: Success Metrics
    print_step(9, "Success Metrics and Impact")
    print("""
Based on the 90-day implementation roadmap, expected results:

Phase 1 (Days 1-30): Quick Wins
  Critical Issues: 12 ‚Üí 0 (-100%) ‚úÖ
  Documentation Debt: 12 ‚Üí 2 (-83%) ‚úÖ
  Terminology Consistency: 65% ‚Üí 95% (+30%) ‚úÖ

Phase 2 (Days 31-60): Structural Improvements
  Navigation Clicks: 3.2 ‚Üí 2.5 (-22%) ‚úÖ
  Content Consistency: 72% ‚Üí 95% (+23%) ‚úÖ
  Code Example Quality: 55% ‚Üí 95% (+40%) ‚úÖ

Phase 3 (Days 61-90): Strategic Enhancements
  Support Tickets: 20/week ‚Üí 15/week (-25%) ‚úÖ
  Search Success Rate: 70% ‚Üí 85% (+15%) ‚úÖ
  Feedback Score: 3.8 ‚Üí 4.5 (+18%) ‚úÖ
  
Real Business Impact:
  üìâ 25% reduction in support tickets
  üìà 40% improvement in findability
  ‚ö° 30% faster developer onboarding
  ‚ú® Consistent, professional documentation
  üéØ Measurable, data-driven improvements
""")
    
    input("\nPress Enter to continue...")
    
    # Summary
    print_header("Demo Complete!")
    
    print("""
üéâ Congratulations! You've seen the complete workflow:

‚úÖ Automated quality scanning
‚úÖ Comprehensive reporting
‚úÖ Metrics tracking with trends
‚úÖ Automated fixing of common issues
‚úÖ Continuous monitoring and alerting
‚úÖ Interactive dashboards
‚úÖ CI/CD integration
‚úÖ Success metrics demonstration

Next Steps:

1. Customize quality_config.yaml for your needs
2. Run initial scan: python doc_quality_automation.py scan
3. Review and fix critical issues
4. Set up CI/CD integration
5. Configure monitoring schedule
6. Track improvements over time

Resources:
- README.md - Complete documentation
- quality_config.yaml - Configuration options
- GitHub Actions workflow - CI/CD examples
- Dashboard - Real-time metrics

Need help? Check the troubleshooting section in README.md

Ready to improve your documentation quality? Let's go! üöÄ
""")
    
    print("\n" + "=" * 80)
    print("  Demo script complete. System ready for production use!")
    print("=" * 80 + "\n")


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nDemo interrupted. Thanks for watching!")
        sys.exit(0)
